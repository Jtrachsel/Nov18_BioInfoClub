---
title: "Primer coverage figures with biopython and ggplot2"
output:
  html_document:
    highlight: pygments
---
####Julian Trachsel
####November 18, 2016

##Hello!

Today we ar going to go through a little bit of the code I used to generate a primer coverage figure from my recent 
[AEM paper](http://aem.asm.org/content/early/2016/09/06/AEM.02307-16.abstract).  

***


The goals for today are not to teach you the details of how to code in python or R, but rather to give you some practice submitting jobs to SLURM on SciNet.  Also, you'll get some examples of what you can accomplish using these languages and to introduce you to some of the packages they have available. 

***

### Outline
1) Login to Scinet
2) Pull some files from github
3) Run a python script
4) Pull the output of this script back to your own computer
5) Use RStudio to generate some figures

***

Special info for Windows users:  

I highly reccommend you download Mobaxterm.  It is a unix shell environment that allows windows users to easily interface with SciNet (or other servers).  If you have mobaxterm working properly everything in this tutorial should work for you.

If you only have PuTTY, you will also need some other Windows scp client such as Winscp or pscp (putty scp)
PuTTY and pscp are available here: [PuTTY download page](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html)


***



### Requirements
Everyone should be able to complete the python portion of this session, but if you want to follow along and run the R code with the group, there are a few things you will need to install:  

1. R
  + [Windows installer](https://cran.r-project.org/bin/windows/base/)
  + [Mac installer](https://cran.r-project.org/bin/macosx/)
  
2. [RStudio](https://www.rstudio.com/products/rstudio/download3/)
  + Follow the link above and download the appropriate installer for your operating system
3. Once you have R and RStudio working we need to install some R packages
  + One of R's great strengths is that many other people have allready written packages to do almost everything you would want to do. We will be using the following packages:
    + ggplot2: A very popular library for generating figures.  Great for mapping variables in your data to graphical parameters
    + ape: **A**nalyses of **P**hylogenetics and **E**volution
    + caper: **C**omparative **A**nalyses of **P**hylogenetics and **E**volution in **R**
    + gridExtra: Used to combine several figures into one cohesive unit 
    + devtools: needed to install ggtree from a github source
    + ggtree: An extension for ggplot2 specifically for plotting phylogenetic trees




#### To install these packages run the following code in R
```{r, eval=FALSE}
# This block installs the packages.  
# There will probably be some weird error messages if you are using windows but it should still work.
# The devtools library needs to be loaded before we can install ggtree from github
install.packages('ggplot2')
install.packages('ape')
install.packages('caper')
install.packages("gridExtra")
install.packages('devtools')
library(devtools)
install_github('Bioconductor-mirror/ggtree')

# This block loads the installed packages so you can use them in your current R session

library(ggtree)
library(ape)
library(ggplot2)
library(ggtree)
library(caper)
```



If you have any issues getting these installed before our meeting, let me know and I'll try to help you out.

***

## Login to Scinet  



Let's run our python script.  

To do this we will login to Scinet. Hopefully everyone can get logged in without too much trouble.....  
Once you are logged in please run the following commands to enter interactive mode, load the required modules, navigate to a working directory of your choosing and pull in the script and sequences from github.  
Open a terminal or putty/mobaxterm:



```{r, engine = 'bash', eval=FALSE}
ssh your.name@scinet-login.bioteam.net
# once you are logged in...

module load git

cd /project/fsepru/ # navigate to our project directory
mkdir jtrachsel  # make your own directory
cd jtrachsel     # go into your own directory, this one is mine
```

***

## Pull files from github


```{r, engine='bash', eval=FALSE}
git clone https://github.com/Jtrachsel/Nov18_BioInfoClub  # pull our script in from github
cd Nov18_BioInfoClub                                      # navigate to the folder containing the script

```


Once you have loaded the required module, and cloned the repository to your own folder we can proceed to run this script.

***

## Run the script  



###DISCLAIMER
#### My code is painfully inefficient, there are undoubtedly better ways to accomplish its goals


Ok, let's make a bash script to submit our python script to SLURM.  
Thanks to Tim we have a nice template already.  

In the folder we downloaded from github, there should be a file called 'SLURM.sh'.  
We need to open this file and make some changes
```{r, engine='bash', eval=FALSE}
nano SLURM.sh
```

This should bring up a script template very similar to what Tim gave us the other week.

```{r, engine='bash', eval=FALSE}


#!/bin/bash

#SBATCH --job-name="funbuts coverage"                # name of the job submitted
#SBATCH -p short                                        # name of the queue you are submitting to
#SBATCH -N 1                                            # number of nodes in this job
#SBATCH -n 40                                           # number of cores/tasks in this job, you get all 20 cores with 2 threads per core with hyperthreading
#SBATCH -t 02:00:00                                     # time allocated for this job hours:mins:seconds
#SBATCH --mail-user=julian.trachsel@ars.usda.gov        # enter your email address to receive emails
#SBATCH --mail-user=julestrachsel@gmail.com
#SBATCH --mail-type=BEGIN,END,FAIL                      # will receive an email when job starts, ends or fails
#SBATCH -o "stdout.%j.%N"                               # standard out %j adds job number to outputfile name and %N adds the node name
#SBATCH -e "stderr.%j.%N"                               # optional but it prints our standard error

# ENTER COMMANDS HERE:

module load python_3
module load muscle
module load openblas

python3 butcoverage.py finalbuts.fasta >> log.txt

#End of file
```


You need to change a few things here:


1. Change: job-name="funbuts coverage" to something of your choosing
2. Enter your own email address here: --mail-user=julian.trachsel@ars.usda.gov
  + And here: --mail-user=julestrachsel@gmail.com

Once you have made the edits exit nano and save the changes:
(press: ctrl+X, then 'y', then 'Enter')

Great job!  

Ok, now we are ready to submit our job to SLURM.  
enter the following command:
```{r, engine='bash', eval=FALSE}
sbatch SLURM.sh
```

Amazing work.  Truly inspiring.  

You should be getting an email informing you that your job has been queued.  This will take approximately 15 minutes to run.  



So while this runs, I will try to walk you through the script and explain what exactly is happening here.

***

## Python script  


This script relies heavily on the Biopython libraries, these are tools that make working with biological sequence data easy.
It is free, open source, and is being actively developed and improved all the time.  I highly reccommend checking it out if you want to do any kind of easy manipulation, filtering, analysis etc. of sequence or other biological data.

[Biopython Home Page](http://biopython.org/wiki/Biopython)  

[Biopython tutorial](http://biopython.org/DIST/docs/tutorial/Tutorial.html)

If you want, you can look at the python script as we go through it, although it is hard to effectively look at with just a basic text editor such as nano.  I will be stepping through it using the PyCharm IDE to make things easier to see

If you are going to be working in python in the future, I highly reccommend you get some kind of IDE (Integrated Development Environment).  Pycharm has a free community edition that has way more functionality than I have ever used.  If you want to check it out use this link: [PyCharm](https://www.jetbrains.com/pycharm/)

```{r, engine='bash', eval=FALSE}
# lets check out the python script we are about to run with nano...
nano butcoverage.py

```
Gross.... hard to see what's going on there.  I'll open the script in Pycharm and it should be easier to look at.  

###What exactly is this script doing?  

#### The main goal of this script is to calculate the number of mismatches three different primer sets have to each gene in the FunGene _but_ database

1. Reads in sequences of all high quality _but_ genes downloaded from the FunGene _but_ database
2. Filters these sequences based on length
3. Removes redundant identical sequences
4. Checks for duplicated sequence ID's and changes any that are duplicated by appending a number
5. Translates the DNA sequences to protein sequences
6. Aligns the protein sequences using Clustal-Omega
7. Generates a DNA codon alignment from the protein alignment and unaligned DNA sequences
8. Checks these alignments for positions in which the vast majority (>95%) of all the sequences contain a gap and removes these positions and the corresponding residues and codons from the unaligned protein and DNA sequences
9. Realigns the edited protein sequences with Clustal-Omega
10. Generates a new codon alignment from the edited protein alignment and DNA sequences
  + This editing step was needed because one of the primer sets targets a region only conserved in a very small number of genes.  Because of this there was a gap that was being introduced into this one binding region, this gap would have thrown off all of the calculations for this primer.  The sequences that were affected by this editing were some of the more distantly related outgroups not critical for this analysis
11. Once the final alignment is generated, this script automatically detects the optimal binding region for each primer.
  + This step is horribly inefficient, you could speed up this script by quite a bit if you just hard-code the alignment coordinates that each primer binds to.
12. Once the optimal binding regions have been identified, the number of mismatches is calculated for each primer against each gene in the alignment.  Additionally some metadata about each sequence is pulled out of the description line in the fasta
  + These data are written as .csv text files, one for each primer set.
13. Once this is done we generate a phylogenetic tree of all the full length sequences using FastTree.  

#### That's all this script does, the next part of the analysis is done in R

***

## Retrieve files from SciNet

Now we need to get our data back from SciNet onto our own computers.  Thanks to Tim, we now know exactly how to do that.
on SciNet type:
```{r, engine='bash', eval=FALSE}
pwd
```

This will tell you exactly where your files are in SciNet.  

REMEMBER THIS PATH.  
Open a new terminal, or exit SciNet.  
Enter the following command:  

```{r, engine='bash', eval=FALSE}
scp -r user.name@scinet-login.bioteam.net:/project/fsepru/YOURFOLDER/Nov18_BioInfoClub ./from_scinet
# be sure you modify this command to have your username as well as the name of the folder you are working from on SciNet

```
This will copy your directory from SciNet and put it in a folder called 'from_scinet'  
You can change the name of this directory if you want.  

Fantastic.  
Now open Rstudio and open the file called 'coverage_trees.R'  



The first thing we need to do is tell RStudio what folder we want to work in (where our data is) and read in our script's results.
```{r}
setwd("~/biclub/finalversion/Nov18_BioInfoClub/")  # Change this to the path where your files from scinet reside
library(ggtree)
library(ape)

###### Reading in the data and the phylogenetic tree  ########
tree <- read.tree("Tree1.nwk")

vital1 <- read.csv("vitalF1_R1_df.primermismatch.csv", header = TRUE, sep = "\t")
vital2 <- read.csv('vitalF2_R2_df.primermismatch.csv', header = TRUE, sep = '\t')
vital3 <- read.csv('vitalF3_R3_df.primermismatch.csv', header = TRUE, sep = '\t')
flint <- read.csv('Flint_df.primermismatch.csv', header = TRUE, sep = '\t')
mybut <- read.csv('Myprimers_df.primermismatch.csv', header = TRUE, sep = '\t')



##### Merging the data into one table and calculating total mismatches per primer set ##########
vtot <- merge(vital1, vital2)
vtot <- merge(vtot, vital3)



vtot$vitalFMM <- with(vtot, pmin(vitalF1_MM, vitalF2_MM, vitalF3_MM))  
vtot$vitalRMM <- with(vtot, pmin(vitalR1_MM, vitalR2_MM, vitalR3_MM))  

metadata <- merge(flint, mybut)
metadata <- merge(metadata, vtot)



metadata$butTOT <- metadata$but672FWD_MM + metadata$but1031REV_MM
metadata$flintTOT <- metadata$BCoATscrF_MM + metadata$BCoATscrR_MM
metadata$vitalTOT <- metadata$vitalFMM + metadata$vitalRMM
colnames(metadata)[1] <- 'SeqID'
colnames(metadata)[4] <- 'funct'

metadata <- metadata[order(metadata$funct),]   # do i need these?
metadata$number <- c(1:length(metadata$SeqID))# ?



```

You can explore the datatables a little bit if you are curious what the output is.  Basically it is a dataframe with But encoding genes as rows, and the columns are the numbers of mismatches each primer we screened has to that gene.  There are also some metadata columns as well containing information of which genes have been biochemically verified.  

***

This next block will generate the figures using the ggtee package


```{r}


### My primers ###

p <- ggtree(tree, layout ="circular", size=.5)
p <- p %<+% metadata
p$data$nonfunct <- grepl('other', p$data$funct)
p$data$confirmed <- grepl('confirmed', p$data$funct)
p$data$greens <- grepl('^1$', p$data$butTOT)

p <- p + 
  geom_hilight(node = 1661, fill = 'grey', alpha = .7) + 
  geom_tippoint(aes(color=butTOT), size=2.7) + 
  scale_color_continuous(name="Total Mismatches", limits=c(0,10),
                         oob=scales::squish, low='darkgreen', high = 'red')


x1 <- p$data$x[p$data$confirmed=='TRUE']
y1 <- p$data$y[p$data$confirmed=='TRUE']
cfirmed<- as.data.frame(cbind(x1,y1))
x11 <- p$data$x[p$data$nonfunct=='TRUE']
y11 <- p$data$y[p$data$nonfunct=='TRUE']
nons<- as.data.frame(cbind(x11,y11))
p1 <- p + geom_point(data=cfirmed, x=x1+0.35, y=y1, fill="blue", shape=24, size=4, stroke=1) +
  geom_point(data=nons, x=x11+0.35, y=y11, fill="yellow", shape=24, size=4, stroke=1)
p1


### Vital Primers ###

p <- ggtree(tree, layout ="circular")
p <- p %<+% metadata
p$data$nonfunct <- grepl('other', p$data$funct)
p$data$confirmed <- grepl('confirmed', p$data$funct)
p$data$greens <- grepl('^1$', p$data$butTOT)

p <- p + 
  geom_hilight(node = 1661, fill = 'grey', alpha = .7) +
  geom_tippoint(aes(color=vitalTOT), size=2.7) + 
  scale_color_continuous(name="Total Mismatches", limits=c(0,10),
                         oob=scales::squish, low='darkgreen', high = 'red')


x1 <- p$data$x[p$data$confirmed=='TRUE']
y1 <- p$data$y[p$data$confirmed=='TRUE']
cfirmed<- as.data.frame(cbind(x1,y1))
x11 <- p$data$x[p$data$nonfunct=='TRUE']
y11 <- p$data$y[p$data$nonfunct=='TRUE']
nons<- as.data.frame(cbind(x11,y11))
p2 <- p + geom_point(data=cfirmed, x=x1+0.35, y=y1, fill="blue", shape=24, size=4, stroke=1) +
  geom_point(data=nons, x=x11+0.35, y=y11, fill="yellow", shape=24, size=4, stroke=1)
p2
### Flint Primers ###

p <- ggtree(tree, layout ="circular", size=.5, alpha=.7)
p <- p %<+% metadata

p$data$nonfunct <- grepl('other', p$data$funct)
p$data$confirmed <- grepl('confirmed', p$data$funct)
p$data$greens <- grepl('^1$', p$data$butTOT)

p <- p  + 
  geom_hilight(node = 1661, fill = 'grey', alpha = .7) + 
  geom_tippoint(aes(color=flintTOT), size=2.7) + 
  scale_color_continuous(name="Total Mismatches", limits=c(0,10),
                         oob=scales::squish, low='darkgreen', high = 'red')


x1 <- p$data$x[p$data$confirmed=='TRUE']
y1 <- p$data$y[p$data$confirmed=='TRUE']
cfirmed<- as.data.frame(cbind(x1,y1))
x11 <- p$data$x[p$data$nonfunct=='TRUE']
y11 <- p$data$y[p$data$nonfunct=='TRUE']
nons<- as.data.frame(cbind(x11,y11))
p3 <- p + geom_point(data=cfirmed, x=x1+0.35, y=y1, fill="blue", shape=24, size=4, stroke=1) +
  geom_point(data=nons, x=x11+0.35, y=y11, fill="yellow", shape=24, size=4, stroke=1)
p3






#
multiplot(p3,p1,p2, ncol = 3)




###################### FIND OUT WHICH NODE!!!!! ##################
# This is really hard to see, but if you zoom in you can get the number of the node we are interested in.
p <- ggtree(tree, layout ="circular", size=.5, alpha=.7)
p <- p %<+% metadata
p +geom_text2(aes(subset=!isTip, label=node), hjust=-.3) + geom_point(data=cfirmed, x=x1+0.35, y=y1, fill="blue", shape=24, size=4, stroke=1) +
  geom_point(data=nons, x=x11+0.35, y=y11, fill="yellow", shape=24, size=4, stroke=1)


#################################### numbers for paper   #######################################
# this section calculates the numbers for primer validation results section
# counts the total number of tips 
library('caper')

total.tips <- sum(p$data$isTip)
good.tips <- clade.members(1661, tree)       # this clade contains all of the verified legitimate But coding sequence

megood <- sum(p$data$butTOT[p$data$node %in% good.tips] < 3)     
vitgood <- sum(p$data$vitalTOT[p$data$node %in% good.tips] < 3)
flintgood <- sum(p$data$flintTOT[p$data$node %in% good.tips] < 3)

metot <- sum(p$data$butTOT[p$data$isTip] < 3)
vittot <- sum(p$data$vitalTOT[p$data$isTip] < 3)
flinttot <- sum(p$data$flintTOT[p$data$isTip] < 3)

metot  # total number of sequences with 2 or fewer total mismatches with my primers
vittot # total number of sequences with 2 or fewer total mismatches with Vital's primers
flinttot # total number of sequences with 2 or fewer total mismatches with Flint's primers


metot - megood # the number of sequences that are likely to be amplified by my primers that are outside of the 'good clade'
vittot - vitgood # the number of sequences that are likely to be amplified by Vital's primers that are outside of the 'good clade'
flinttot-flintgood # the number of sequences that are likely to be amplified by Vital's primers that are outside of 'the good clade'


# now the ratios
megood/metot  *100  # 94% of the sequences likely to amplify with my primers are within the 'good clade'
vitgood/vittot *100 # 44% of the sequences likely to amplify with Vital's primers are within the 'good clade'
flintgood/flinttot *100 # 100%


```

Well done everyone.  I hope this was good practice submitting jobs to SLURM and I hope that it has inspired you to check out some of the packages available in Python and R.
